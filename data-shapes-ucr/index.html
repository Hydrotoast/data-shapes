<!DOCTYPE html>
<html>
<head>
	<title>RDF Data Shapes Use Cases and Requirements</title>
	<meta charset='utf-8'>
	<script src='http://www.w3.org/Tools/respec/respec-w3c-common'
	async class='remove'></script>
	<script class='remove'>
		var respecConfig = {
			specStatus: "FPWD",
			shortName:  "shapes-ucr",
			editors: [{   name:       "Simon Steyskal",
			url:        "http://steyskal.info/",
			company:    "WU Vienna",
			companyURL: "http://www.wu.ac.at/infobiz/" },
			{   name:       "Karen Coyle",
			url:        "http://kcoyle.net/",
			company:    "DCMI",
			companyURL: "http://dublincore.org/" }

			],
			wg:           "RDF Data Shapes Working Group",
			wgURI:        "https://www.w3.org/2014/data-shapes",
			wgPublicList: "public-rdf-shapes",
			wgPatentURI:  "http://www.w3.org/2004/01/pp-impl/73865/status",
			localBiblio:  {
				"xyz": {
					title:    "Sample Custom Reference",
					href:     "http://example.org/",
					"authors": [
					"Alistair Cockburn"
					],
					publisher: "xyz"
				}
			}
		};
	</script>
</head>
<body>
	<section id='abstract'>
		<p>To foster the development of the RDF Data Shapes specification, this document includes a set of user stories, use cases, and requirements 
			that motivate a simple read-write Linked Data architecture, based on HTTP access to web resources that describe their state using RDF. 
			The starting point for the development of these use cases is a collection of user stories that provide realistic examples describing how 
			people may use structural constraints to validate RDF instance data. The use cases themselves are captured in a narrative style that 
			describes a behavior, or set of behaviors based on, and using scenarios from, these user stories. Note, that this document avoids 
			the use of any specific vocabulary that might be introduced by the <abbr title="RDF Data Shapes">SHAPES</abbr> specification.</p>
		</section>

		<section id='sotd'>

		</section>

		<!-- taken from http://www.w3.org/2014/data-shapes/charter -->
		<section class='informative'>
			<h1>Scope and Motivation</h1>
			<p>
				One motivation for this work is Application Integration, where different software components, potentially maintained by different organizations, need to function together smoothly. As a everyday example, imagine an international company with a dozen divisions, each providing a feed of their Human Resources data to authorized users. Different divisions might use different software to produce their feeds, and there might be many distinct applications which consume the data, ranging from an employee phone book to a hiring-compliance monitoring system. 
			</p>
			<p>
				While systems like this are built and maintained around the world today, their complexity often becomes a problem. Not only are the systems expensive and sometimes unpleasant to maintain, but changing data fields and adding new applications can grow to be practically impossible. An "RDF Data Shapes" standard would help manage the complexity, greatly reducing the cost and hassle, by separating components while still allowing them to work together.
			</p>
			<p>
				Specifically, in this example, an RDF Data Shapes Language would allow: 
				<ul>
					<li>Developers of each data-consuming application could define the shapes their software needs to find in each feed, in order to work properly, with optional elements it can use to work bette</li>
					<li>When these developers want to modify their software, they can define new shapes they require.</li>
					<li>Management can offer guidance in the relative priorities of outputing particular shapes, based on the application(s) that use them. There might be target goals and deadlines.</li>
					<li>Developers of data-providing systems can read the shape definitions (and possibly related RDF Vocabulary definitions) to learn what they need to provide.</li>
					<li>Data providers can also validate their data against the definitions, to see if they are producing the right information. (Of course, this doesn't ensure the data is correct, just that it's the right shape.) </li>
					<li>Data consumers can validate incoming data against the expected shapes, to make sure they are getting the kind of data they were expecting. This can be done manually from time to time, or automatically on all data. This kind of validation is particularly important if producers and consumers keep updating their software to use new shapes to meet changing requirements. </li>
					<li>Intermediate systems can, in some cases, be written to convert data written to match one shape into data which matches a different shape. </li>
					<li>Some systems may be able to automatically generate user interface elements (eg HTML forms) and/or data bindings based on shapes.</li>
					<li>There may be optimizations in data processing possible when the data is known to conform to a single declared shape. </li>
				</ul>
			</p>
			<p>
				In all cases, the <em>semantics</em> of the data are determined by RDF and the vocabularies specified by the shape, so if the shapes match, the systems can reasonably be expected to interoperate correctly. 
			</p>
			<p>
				While RDF Data Shapes are expected to have immediate everyday utility, as illustrated above, they have even wider potential applicability, ranging in scale. At the large end, RDF Data Shapes might be used by loosely-knit communities, where data is provided by organizations which are not under any central authority, such as charities and researchers around the world concerned with quality-of-life measures. At the small end, RDF Data Shapes might be used within a mobile application environment to provide interoperability among independent sensor modules and tools for analyzing and acting on sensor results. The common thread is that RDF Data Shapes allow a loose coupling, where independently maintained elements of an overall system can reliably and comfortably interoperate. 
			</p>
		</section>

		<section class='informative'>
			<h1>Organization of this Document</h1>
			<p>This document is organized as follows:</p>
			<ul>
				<li><b><a href="#usecases" title="Use Cases">Use Cases</a></b> are
					used to capture and model functional requirements. Use cases
					describe the system’s behavior under various conditions [[COCKBURN-2000]],
					cataloging who does what with the system, for what purpose, but
					without concern for system design or implementation. Each use case is identified by a
					reference number to aid cross-reference from other documentation. A variety of styles may be used to capture use cases,
					from a simple narrative to a structured description with actors,
					pre/post conditions, and non-functional requirements
					raised by the use case.</li>
				</ul>

	<!--<ul>
		<li><b><a href="#userstories" title="User Stories">User Stories</a></b>
			capture statements about system requirements written from a user
			or application perspective. They are typically lightweight and
			informal and can run from one line to a paragraph or two
			(sometimes described as an 'epic') [[xyz]]. 
			This document redacts a number of user stories around the theme of read/writeable linked data.
			Analysis of each user story reveals a
			number of (functional) use cases and other non-functional
			requirements. See <em>Device API Access Control Use Cases and Requirements</em> [[xyz]] for a good example
			of user stories and their analysis.</li>
	</ul>
	<ul>
		<li><b><a href="#usecases" title="Use Cases">Use Cases</a></b> are
			used to capture and model functional requirements. Use cases
			describe the system’s behavior under various conditions [[xyz]],
			cataloging who does what with the system, for what purpose, but
			without concern for system design or implementation. Each use case is identified by a
			reference number to aid cross-reference from other documentation;
			use case indexing in this document is based on rdb2rdf
			use cases [[xyz]]. A variety of styles may be used to capture use cases,
			from a simple narrative to a structured description with actors,
			pre/post conditions, step-by-step behaviors (as in <em>POWDER:
			Use Cases and Requirements</em> [[xyz]]), and non-functional requirements
			raised by the use case.</li>
		</ul>-->
		<ul>
			<li><b><a href="#requirements" title="Requirements">Requirements</a></b>
				list functional and non-functional or quality requirements, and the use cases
				they may be derived from. This approach is exemplified in the <em>Use Cases and Requirements for the Data
				Catalog Vocabulary</em> [[DCAT-UCR]].</li>
			</ul>
		</section>

		<section>
			<h1 id="userstories">User Stories</h1>
			<!-- User Story 1 -->
			<section>
				<h2><dfn>S1</dfn>: The model's broken!</h2>
				<p>
					Validate RDFS (maybe also OWL) models
					The basic issue here is to ensure that the right kind of information is given for each property (or class) in the model, for example, to require that each property has to have a domain, or that classes have to be explicitly stated to be under some decomposition.
					Input data: the RDF representation of an RDFS (or OWL) ontology
					Input ontology: the ontology that represents RDFS (or OWL) syntax 
				</p>
				<section>
					<h3>Summary:</h3>
					<p>Requires the ability to check whether certain information is given/available for a property or class.</p>
				</section>
				<section>
					<h3>Related Requirements:</h3>
					<p> ... </p>
				</section>
			</section>

			<!-- User Story 2 -->
			<section>
				<h2><dfn>S2</dfn>: What's the name of that person?</h2>
				<p>
					For a tool to build list of names for named entity resolution of people to work correctly, every person has to have one or more names specified, each of which is a string. Constraints can be used to verify that particular sets of data have such names for each person.
				</p>
				<section>
					<h3>Summary:</h3>						
					<p>Requires the ability to check the cardinality of properties as well as the type of its values.</p>
					<p>Related to: <a>S8</a>,<a>S11</a>,<a>S23</a>, <a>S37</a></p>
				</section>
				<section>
					<h3>Related Requirements:</h3>
					<p> ... </p>
				</section>
			</section> 

			<!-- User Story 3 -->
			<section>
				<h2><dfn>S3</dfn>: Communicating back to users, kindly</h2>
				<p>
					Rather than rejecting or having yes/no, and discouraging users and rejecting a lot of data, have a number of responses that inform users of ways they could improve their data, while still accepting all but the truly unusable data. This requires levels of "validation".
				</p>
				<section>
					<h3>Summary:</h3>						
					<p> Requires the ability to return responses appropriate to the condition, not just "pass/fail."</p>
				</section>
				<section>
					<h3>Related Requirements:</h3>
					<p> ... </p>
				</section>
			</section>

			<!-- User Story 4 -->
			<section>
				<h2><dfn>S4</dfn>: Issue repository</h2>
				<p>
					An LDP Container &lt;http://PendingIssues&gt; accepts an IssueShape with a status of "assigned" or "unassigned". The LDP Container is an interface to a service storing data in a conventional relational database. The shapes are "closed" in that the system rejects documents with any triples for which it has no storage. The shapes validation process (initiated by the receiving system or a sender checking) rejects any document with "extraneous" triples.
				</p>
				<p>
					Any node in the graph may serve multiple roles, e.g. the same node may include properties for a SubmittingUser and for an AssignedEmployee.
				</p>
				<p>
					Later the issue gets resolved and is available at &lt;http://OldIssues&gt; without acquiring new type arcs. The constraints for &lt;http://PendingIssues&gt; are different from those for Issues at &lt;http://OldIssues&gt; 
				</p>	
				<section>
					<h3>Summary:</h3>						
					<p>Requires the ability to associate more than one shape to the same graph or node.</p>
				</section>
				<section>
					<h3>Related Requirements:</h3>
					<p> ... </p>
				</section>		
			</section>

			<!-- User Story 5 -->
			<section>
				<h2><dfn>S5</dfn>: Closed-world recognition (EPIM ReportingHub)</h2>
				<p>
					EPIM Project - petroleum operators on the Norwegian continental shell need to produce environment reports of what chemicals were dumped into the sea and gases to the air. There is a need for access rules on what operators can see what data from what oil and gas fields, and for complex constraints to run during import of XML files. SPIN was used to represent and evaluate those constraints.
				</p>
				<p>
					This is an example of very complex constraints that require many features from SPARQL to represent model-specific scenarios, including the comparison of incoming values against a controlled fact base, transformations from literal values to URIs, string operations, date comparisons etc. User-defined SPIN functions were used to make those complex queries maintainable.
				</p>
				<p>
					Details: <a href="https://www.w3.org/2014/data-shapes/wiki/EPIM_ReportingHub">EPIM ReportingHub</a>
				</p>
				<section>
					<h3>Summary:</h3>						
					<p>Requires the expressibility of complex constraints that include e.g. value transformations, string operations, date comparisons, etc.</p>
				</section>
				<section>
					<h3>Related Requirements:</h3>
					<p> ... </p>
				</section>
			</section>

		<!-- User Story 6 
		<section>
			<h2><dfn>S6</dfn>: Closed-world recognition for e.g. for partial ontology import</h2>
				<p>
					Importing all of an ontology is not always a good practice. When an ontology is imported it is often the case that many concepts and properties will be irrelevant to the needs at hand. In addition transitive imports can lead to increased "Ontology Glut". An increasingly popular practice is to not do any imports but to explicitly declare the use of non-imported resources with rdfs:definedIn to provide the provenance to the authoritative defining point of the resource. Alternatively some way to constrain imports to avoid ontology glut might be useful.
				</p>
				<p>
					SPIN currently uses owl:imports to include other graphs. If no owl:imports statement is present, then the engine will not execute constraints stored in the remote schema. It is perfectly fine to have local copies of classes and properties defined elsewhere, without requiring the full contract. This is a common scenario in controlled environments, not the full Web.
				</p>
				<section><h3>Summary:</h3>						<p><strong>still has open issue</strong></p>
						<p>Must be able to define and/or import only those parts of an ontology that are needed for the validation.</p>
					</div>
		</section>
				
		<!-- User Story 7 (was deleted) --> 

		<!-- User Story 8 -->
		<section>
			<h2><dfn>S8</dfn>: Checking RDF node type</h2>
			<p>
				It is often necessary or desirable to check whether certain property values (or more general: RDF nodes) are of a specific node type (IRI, BlankNode or Literal and all combinations thereof). Often the intention is to state that a given property shall only have IRIs but not BlankNodes.
			</p>
			<p>
				Two examples from the VOID namespace, <a href="http://www.w3.org/TR/void/#dumps">void:dataDump</a> and <a href="http://www.w3.org/TR/void/#example-resource">void:exampleResource</a> declare an rdfs:range of rdfs:Resource, but the intention is to only support IRI resources. 
			</p>
			<p>
				The DCAT namespace has similar examples, where only IRI nodes are permitted: dcat:landingPage, dcat:accessURL and dcat:downloadURL. The declared ranges are foaf:Document or rdfs:Resource. The foaf:Document case is interesting, shows that people might want to specify both the value’s class (foaf:Document) and node type (URI only). 
			</p>
			<p>
				SPARQL includes the built-ins isIRI, isBlank, isLiteral for those checks. 
			</p>
			<section>
				<h3>Summary:</h3>						
				<p>Requires the possibility to constrain the value of a property. E.g. check whether it is an IRI, a literal, a blank node, or some combination of those. </p>
				<p>Related to: <a>S2</a>,<a>S11</a>,<a>S23</a>, <a>S37</a></p>
			</section>
			<section>
				<h3>Related Requirements:</h3>
				<p> ... </p>
			</section>
		</section>
		<!-- User Story 9 
		<section>
			<h2><dfn>S9</dfn>: Contract time intervals</h2>
				<p>
					OMG time ontology adopted by FIBO. end date *exists* but may not be specified. Some contracts (bonds) have an end date. 
				</p>
				<section><h3>Summary:</h3>		<p><strong>still has open issue</strong></p>
						<p>Validation must allow for (momentarily) unspecified values. For example, an end date may be assumed but is not specified at this time.</p>
					</div>
				</section> -->

		<!-- User Story 10 
		<section>
			<h2><dfn>S10</dfn>: card >= 0</h2>
				<p>
					Mention a property in a card>= 0 restriction, just to indicate an expectation that it will (or might) be there without requiring that it be there.
				</p>
				<p>
					(ericP: may I replace this with a story with requirements for different cardinalities, including fixed cardinalities > 1 (2 comes up a lot, e.g. two biological parents)? I propose:)
				</p>
				<p>
					Clinical data requires specific cardinality constraints, e.g.
					<ul>
						<li>zero or one (optional) birth date.</li>
						<li>zero or more lab tests.</li>
						<li>one active patient marker.</li>
						<li>one or more emergency contact.</li>
						<li>two biological parents.</li>
					</ul>
				</p>
			</section>-->

			<!-- User Story 11 -->
			<section>
				<h2><dfn>S11</dfn>: Model-Driven UI constraints</h2>
				<p>
					Need to have constraints provide model-driven validation of permissible values in user interfaces. A number of solutions and applications have been deployed which use SPIN to check constraints on permissible values to user interfaces. This overcomes the software debt that comes from using javascript that can readily become out-of-sync with the underlying models.
				</p>
				<p>
					The major requirement here is a declarative model of: 
					<ul>
						<li>which properties are relevant for a given class/instance?</li>
						<li>what is the value type of those properties?</li>
						<li>what is the valid cardinality (min/maxCount)?</li>
						<li>what is the interval of valid literal values (min/maxValue)?</li>
						<li>any other metadata typically needed to build forms with input widgets.</li>
					</ul>
				</p>
				<p>
					A meta-requirement here is to be able to make use of the information above without having to run something like SPARQL queries, i.e. the model should be sufficiently high level so that all kinds of tools can use that information. However, at the same time there are many advanced constraints that need to be validated (either on server or client) before a form can be submitted. These constraints are not necessarily "structural" information, but rather executable code that returns error messages. 
				</p>
				<section>
					<h3>Summary:</h3>						
					<p>Requires the ability to declare and constrain permitted values for properties, as well as their cardinalities, in an abstract
						and "high-level" fashion.</p>

						<p>Related to: <a>S2</a>,<a>S8</a>,<a>S23</a>, <a>S37</a></p>				
					</section>
					<section>
						<h3>Related Requirements:</h3>
						<p> ... </p>
					</section>
				</section>

		<!-- User Story 12 
		<section>
			<h2><dfn>S12</dfn>: App Interoperability</h2>
				<p>
					For example, cimba.co acts as a decentralized twitter, operating over LDP. For another app to interoperate, it needs to know what data shapes cimba reads and write. This is currently documented with diagrams and sparql templates. The SPARQL is fairly complex and hard to read, and it seems like another language might make it easier to write interoperable programs. 
				</p>
			</section>	-->

			<!-- User Story 13 -->
			<section>
				<h2><dfn>S13</dfn>: Specification and validation of metadata templates for immunological experiments</h2>
				<p>
					Systems Biology is playing an increasingly important role in unraveling the complexity of human immune responses. A key aspect of this approach involves the analysis and integration of data from a multiplicity of high-throughput immune profiling methods to understand (and eventually predict) the immunological response to infection and vaccination under diverse conditions. To this end, the Human Immunology Project Consortium (HIPC) was established by the National Institute of Allergy and Infectious Diseases (NIAID) of the US National Institutes of Health (NIH). This consortium generates a wide variety of phenotypic and molecular data from well-characterized patient cohorts, including genome-wide expression profiling, high-dimensional flow cytometry and serum cytokine concentrations. The adoption and adherence to data standards is critical to enable data integration across HIPC centers, and facilitate data re-use by the wider scientific community.
				</p>
				<p>
					In collaboration with ImmPort, we have developed a set of spreadsheet-based templates to capture the metadata associated with experimental results such as Flow Cytometry results and Multiplex Bead Array Assay (MBAA) results. These templates contain metadata elements that are either required or optional, but importantly, define the value of the field to specific datatypes (e.g. string, integer, decimal, date) that may be restricted by length or to a regular expression pattern, and limited to specific categorical values or terminology trees/class expressions of a target ontology, especially those drawn from existing ontologies such as Cell Ontology (CL) and Protein Ontology (PO). Once filled out, these spreadsheets are programmatically validated. The values are then stored in a database and are used to power web applications and application programming interfaces.
				</p>
				<p>
					Given the rapid change in the kinds of experiments performed and the evolving requirements concerning relevant metadata, it is crucial that a language to define these metadata constraints enable us to define different sets of metadata fields and values sets in a modular manner. In addition to HIPC, there are other immunology consortia that might involve different requirements as to how data templates should be defined according to specific needs. It should be relatively straightforward to substitute one set of shape expressions for another. It is also important that the shapes themselves are versioned and the results of validation record the version of the shape expression. It should be possible to validate data using any set of developed shapes.
				</p>
				<p>
					Ideally, the shapes language should be readable by computers in order to automatically generate template forms with restriction to specified values. Moreover, libraries and tools to construct and validate templates and their instance data should be readily available. 
				</p>
				<section>
					<h3>Summary:</h3>						
					<p>Requires the possibility to define shapes for a specific node in a modular manner, i.e. defining different sets of metadata fields and value sets.</p>
					<p>Requires the availability of version information of shapes thus, the results of validation shall record the version of the triggered shape expression.</p>
					<p>Related to(regarding overall constraint requirements): <a>S2</a>,<a>S8</a>,<a>S23</a>, <a>S37</a> </p>
				</section>
				<section>
					<h3>Related Requirements:</h3>
					<p> ... </p>
				</section>
			</section>		

		<!-- User Story 14 
		<section>
			<h2><dfn>S14</dfn>: Object Reconciliation</h2>
				<p>
					As an aid in data integration activities, it would be nice if shapes could flexibly state conditions by which to check that identity of objects has been correctly recorded; that is, check conditions under which 2 objects in a KB should explicitly represent the same real-world thing. For example (movies domain), I'd like to say: 
				</p>
				<ul>
					<li>
						<code>if source1.movie.title is identical to source2.film.title AND source1.movie.release-date.year is close (say, < 2 years difference) to source2.film.initial-release then it should be stated that they are the same movie </code> OR
					</li>
					<li>
						<code>if source1.movie.directors has the same set of values as source2.film.directed-by AND source1.movie.title is highly similar to source2.film.title then it should be stated that they are the same movie </code> OR
					</li>
					<li>...</li>
				</ul>

			</section>	-->

			<!-- User Story 15 -->
			<section>
				<h2><dfn>S15</dfn>: Validation of Dataset Descriptions</h2>
				<p>
					Access to consistent, high-quality metadata is critical to finding, understanding, exchanging, and reusing scientific data. The W3C Health Care and Life Sciences Interest Group (HCLSIG) has developed consensus among participating stakeholders on key metadata elements and their value sets for description of HCLS datasets. This <a href="http://tinyurl.com/hcls-dataset-description">specification</a>, written as a W3C note, meets key functional requirements, reuses existing vocabularies, is expressed using the Resource Description Framework (RDF). It provides guidance for minimal data description, versioning, provenance, statistics. We would like to use RDF Shapes to specify these constraints and validate the correctness of HCLS dataset descriptions.
				</p>
				<p>
					The specification defines a 3 component model for summary,versioning, and distribution-level descriptions. Each component has access to a specific set of metadata elements and these are specified as MUST, SHOULD, MAY, and MUST NOT. As such there are different conformance criteria for each level. Metadata values are either unrestrained rdfs:Literals, constrained rdfs:Literals, URIs with a specified URI pattern, or instances of a specified URI-identified type, or a disjunction of URI-specified types.
				</p>
				<p>
					Cardinalities and ranges are covered by all existing proposals, so I guess the interesting bit here is how to represent that certain constraints only apply in certain contexts ("levels: summary, version, distribution"). 
				</p>
				<section>
					<h3>Summary:</h3>		
					<p>Requires the functionality to restrict application of constraints to certain contexts.</p>
					<p>Requires expressibility of cardinality constraints and property value restrictions.</p>
					<!--<p>Validation must take place within a context that defines the set of rules to be applied and the response codes returned on specific conditions.</p>-->
				</section>
				<section>
					<h3>Related Requirements:</h3>
					<p> ... </p>
				</section>
			</section>	

			<!-- User Story 16 -->
			<section>
				<h2><dfn>S16</dfn>: Constraints and controlled reasoning. We need both!</h2>
				<p>
					A use-case we were facing recently and have discussed in <a href="https://ai.wu.ac.at/~polleres/publications/sche-etal-2014ConfigWS.pdf">[1]</a>, was revolving around the integration of distributed configurations (i.e. object-oriented models) with RDFS and SPARQL. 
				</p>
				<p>
					In this particular use-case we had to assume both Unique Name Assumption (UNA) and Closed World Assumption (CWA) for our ontologies, since the models (i.e. configurations) those ontologies were derived from were generated by product configurators which impose both UNA and CWA. Since neither RDFS or OWL impose UNA/CWA we had to come up with some workarounds which were basically: 
					<ul>
						<li>
							<strong>UNA 2.0:</strong> all URIs are treated as different, unless explicitly stated otherwise by owl:sameAs (UNA 2.0 because in general, if two URIs are different and the ontology they are contained in is assumed to obey the UNA then they cannot be connected via owl:sameAs). 
						</li>
						<li>
							<strong>CWA:</strong> we assumed to know every existing individual of local configurations and directly connected individuals from other local configurations, thus an absence of a certain individual in the local configuration means that it does not exist. 
						</li>
					</ul>
				</p>
				<p>
					As mentioned earlier, we used SPARQL to perform query tasks on the global schema as well as to check simple integrity constraints by translating e.g. cardinality restrictions into ASK queries.
				</p>

				<p>
					One major problem which arose based on our workaround to impose UNA was, that SPARQL is unaware of the special semantics of owl:sameAs. Which means that especially if one wants to use counting aggregates, one usually wants to count the number of real-objects and not the number of URIs referring to it. As an example we defined two SPARQL queries which should count the number of subnets of a certain system:

					<pre id="example1" class="example highlight">
						SELECT (COUNT(DISTINCT ?subnet) AS ?numberofsubnets)
						WHERE {
						?subnet a ontoSys:Subnet .
					}
					# result: numberofsubnets = 3 
				</pre>
				<pre id="example2" class="example highlight">
					SELECT (COUNT(DISTINCT ?subnet) AS ?numberofsubnets)
					WHERE {
					?subnet a ontoSys:Subnet .
					# first subquery
					{ SELECT ?subnet ?first
					WHERE {
					?subnet ((owl:sameAs|^owl:sameAs)*) ?first .
					OPTIONAL {
					?notfirst ((owl:sameAs|^owl:sameAs)*) ?first .
					FILTER (STR(?notfirst) < STR(?first))}
					FILTER(!BOUND(?notfirst))}
				}
			}
			# result : numberofsubset = 1
		</pre>
	</p>

	<p>
		Obviously <a href="#example2">Example 2</a> is way more ugly than <a href="#example1">Example 1</a>, especially due to some nasty path expressions which are necessary to traverse through potential owl:sameAs chains. Other approaches such as replacing those chains with pivot-identifiers in a potential pre-processing step are not feasible since we actually want to keep the different identifiers separate in the data for particular use-cases. 
	</p>
	<section>
		<h3>Summary:</h3>						
		<p>Requires support of unique name assumption, such that each unique IRI is assumed to represent a unique entity.</p>
		<p>Requires possibility to encapsulate verbose constraint definitions into macros thus, allow their reuse in other shapes as well as increase readability of shape expressions.</p>
	</section>
	<section>
		<h3>Related Requirements:</h3>
		<p> ... </p>
	</section>
</section>	

		<!-- User Story 17 
		<section>
			<h2><dfn>S17</dfn>: Specify subsets of data</h2>
				<p>
					Have a lightweight way to refer to a part of a data set, based on the shapes. This could be used for entitlements as well ("you can see AML/KYC shape for this class", "You can only see the identification shape for this class") 
				</p>
			</section>	-->

		<!-- User Story 18
		<section>
			<h2><dfn>S18</dfn>: Scope of Export</h2>
				<p>
					Starting from a given KB object (individual), I want to export a bunch of related stuff. Use shapes to specify the paths / conditions by which the stuff to be exported can be selected.
				</p>
			</section>		 -->

		<!-- User Story 19 
		<section>
			<h2><dfn>S19</dfn>: Query Builder</h2>
				<p>
					Various tools are contributing data to a triple store. A Query Builder wants to know the permitted or likely shapes of the data over which the generated queries must run, so that the end user can be presented with a nice interface prompting for likely predicates and values. Since the data is dynamic, this is not necessarily the same as the shape that could be reverse engineered from the existing data. The Query Builder and the data-producing tools are not provided by the same team - the Query Builder team has very limited control over the data being produced. The source of the data might not provide the necessary shape information, so we need a way for the Query Builder team (or a third party) to be able to provide the shape data independently. See also <a href="https://www.w3.org/2014/data-shapes/wiki/Ontology-Driven_Forms">Ontology-Driven Forms</a> and <a>S11</a>. 
				</p>
			</section>	-->

		<!-- User Story 20 
		<section>
			<h2><dfn>S20</dfn>: Creation Shapes</h2>
				<p>
					A client creating a new resource by posting to a Linked Data Platform Container [2] wants to know the acceptable properties and their values, including which ones are mandatory and which optional. Note that this creation shape is not necessarily the same as the shape of the resource post-creation - the server may transform some values, add new properties, etc. [2] http://www.w3.org/TR/ldp/#ldpc
				</p>
				<p> 
					See the ongoing discussion at http://lists.w3.org/Archives/Public/public-data-shapes-wg/2014Nov/0160.html with hints at a solution based on named graphs. Other solutions with stand-alone shapes have been proposed as well as an option to select constraints based on decorations (annotations) 
				</p>
			</section>	-->

			<!-- User Story 21 -->
			<section>
				<h2><dfn>S21</dfn>: SKOS Constraints</h2>
				<p>
					The well-known SKOS vocabulary defines constraints that are outside of the expressivity of current ontology languages. They can be expressed using SPARQL built-ins, e.g. via SPIN. Examples include:
					<ul>
						<li>make sure that a resource has at most one preferred label for a given language</li>
						<li>preferred labels and alternative labels must be disjoint</li>
					</ul>
					Details: <a href="https://www.w3.org/2014/data-shapes/wiki/SKOS_Constraints">SKOS Constraints</a>
				</p>
				<section>
					<h3>Summary:</h3>		
					<p>Requires the possibility to define complex constraints including ones on property/value pairs.</p>
					<p>Related to: Cardinality constraints, constraints over properties of the same node.</p><</section>
					<section>
						<h3>Related Requirements:</h3>
						<p> ... </p>
					</section>
				</section>
				<!-- User Story 22 -->
				<section>
					<h2><dfn>S22</dfn>: RDF Data Cube Constraints</h2>
					<p>
						The Data Cube Vocabulary provides a means to publish multi-dimensional data, such as statistics, on the web in such a way that it can be linked to related data sets and concepts. While the bulk of the vocabulary is defined as an RDF Schema, it also includes <a href="http://www.w3.org/TR/vocab-data-cube/#wf-rules">integrity constraints</a>:
					</p>
					<p>
						Each integrity constraint is expressed as narrative prose and, where possible, a SPARQL ASK query or query template. If the ASK query is applied to an RDF graph then it will return true if that graph contains one or more Data Cube instances which violate the corresponding constraint.
					</p>
					<p>
						Using SPARQL queries to express the integrity constraints does not imply that integrity checking must be performed this way. Implementations are free to use alternative query formulations or alternative implementation techniques to perform equivalent checks.
					</p>
					<pre id="example3" class="example highlight">
						#Every qb:DataStructureDefinition must include at least one declared measure
						ASK {
						?dsd a qb:DataStructureDefinition .
						FILTER NOT EXISTS { ?dsd qb:component [qb:componentProperty [a qb:MeasureProperty]] }
					}
				</pre>
				<section>
					<h3>Summary:</h3>						
					<p>Requires support of RDF Data Cube Integrity Constraints</p>
					<p>Related to: Cardinality constraints, constraints over properties of the same node, property value restrictions.</p>
				</section>
				<section>
					<h3>Related Requirements:</h3>
					<p> ... </p>
				</section>
			</section>	

			<!-- User Story 23 -->
			<section>
				<h2><dfn>S23</dfn>: schema.org Constraints</h2>
				<p>
					Developers at Google have created a validation tool for the well-known schema.org vocabulary for use in Google Search, Google Now and Gmail. They have found that what may seem like a potentially infinite number of possible constraints can be represented quite succinctly using existing standards like the SPARQL query language and serialized as RDF. 
				</p>
				<p>
					<ul>
						<li>On <strong>schema:Person</strong>: Children cannot contain cycles, Children must be born after the parent, deathDate must be after birthDate </li>
						<li>On <strong>schema:GeoCoordinates</strong>: longitude must be between -180 and 180, latitude between -90 and 90 </li>
						<li>On <strong>various</strong>: email address must match a certain regular expression </li>
						<li>On <strong>schema:priceCurrency</strong>, currenciesAccepted: Currency code must be from a given controlled vocabulary </li>
						<li>On <strong>schema:children</strong>, colleagues, follows, knows, parents, relatedTo, siblings, spouse, subEvents, superEvents: Irreflexitity </li>
					</ul>
				</p>
				<p> Solution from the <a href="http://www.w3.org/2001/sw/wiki/images/0/00/SimpleApplication-SpecificConstraintsforRDFModels.pdf">Google Paper (JSON-LD)</a>, replacing boardingTime with departureTime: 
					<pre id="example4" class="example highlight">
						#Boarding passes will only be shown in Google Now for flights which occur at a future date: 
						{
						"@context": {...},
						"@id": "schema:FlightReservation",
						"constraints": [{
						"context": "schema:reservationFor",
						"constraint": "ASK WHERE {?s schema:departureTime ?t. FILTER(?t > NOW())}",
						"severity": "warning",
						"message": "A future date is required to show a boarding pass.",
					}]
				}
			</pre>
		</p>
		<section>
			<h3>Summary:</h3>						<!--<p>(These seem to be covered elsewhere, and aren't limited to schema.org)</p>-->
			<p>Requires support of schema.org constraints.</p>
			<p>Related to: <a>S2</a>,<a>S8</a>,<a>S11</a>, <a>S37</a></p>
			<p>Related to: Property value restrictions.</p>
		</section>
		<section>
			<h3>Related Requirements:</h3>
			<p> ... </p>
		</section>
	</section>	

	<!-- User Story 24 -->
	<section>
		<h2><dfn>S24</dfn>: Open Content Model</h2>
		<p> 
			Suppose there is a need to integrate similar information from multiple applications and that the application owners have gotten together and defined an RDF representation for this information. However, since the applications have some differences, the application owners can only agree on those data items that are common to all applications. The defined RDF representation includes the common data items, and allows the presence of other undefined data items in order to accommodate differences among the applications. In this situation, the RDF representation is said to have an open content model. In fact, one of the attractive features of RDF technology is that it readily enables open content models.
		</p>
		<p>
			For example, the <a href="http://open-services.net/bin/view/Main/CmSpecificationV2">OSLC Change Management (CM)</a> specification specifies a very minimal representation for Change Requests (e.g. bug reports). A large software development organization may use several different change management tools, e.g. Bugzilla, Jira, and ClearQuest, each with their own proprietary resource format. The OSLC CM specification provides a way to process change management information in a uniform way, independently of the tool that hosts it. However, there may also be interesting differences in the type of information hosted by each tool. OSLC therefore specifies an open content model which allows implementations to extend the base representation with additional content. This content is represented as additional RDF properties on the resources. Furthermore, it is very common for change management tools to partition their resources into defined projects which restrict who can access the resources and which define custom attributes on the resources. Here the term custom attribute refers to an attribute that is not defined out-of-the-box in the tool. The tool administrators customize the tool by defining custom attributes, typically on a per-project basis. For example, one project might add a customer reference number while another might add a boolean flag indicating if there is an impact to the online documentation. These custom attributes also appear as additional RDF properties of the resources.
		</p>
		<p>
			OSLC specifications typically define one or more RDF types. For example, the RDF type for change requests is oslc_cm:ChangeRequest where the prefix oslc_cm is &lt;http://open-services.net/ns/cm#&gt;. The RDF representation of an OSLC change request contains a triple that defines its type as oslc_cm:ChangeRequest, triples that define RDF properties as described in the OSLC CM specification, and additional triples that correspond to tool-specific or project-specific custom attributes. Note that the addition of custom attributes does not require the definition of a new RDF type. Furthermore the RDF properties used to represent custom attributes may come from any RDF vocabulary. In fact, tool administrators are encouraged to reuse existing RDF properties rather than define synonyms.
		</p>
		<p>
			Since the shape of a resource may depend on the tool that hosts it, or the project that hosts it within a tool, but the RDF type of the resource may not depend on the tool or project, there is in general no way to navigate to the shape given only its RDF type. The <a href="http://www.w3.org/Submission/shapes/"> OSLC Resource Shapes</a> specification provides two mechanisms for navigating to the appropriate shape. First, the RDF property oslc:resourceShape where oslc: is &lt;http://open-services.net/ns/core#&gt; may be used to link a tool or project description to a shape resource. Second, the RDF property oslc:instanceShape may be used to link a resource to its shape. 
		</p>
		<p>	
			See <a href="https://www.w3.org/2014/data-shapes/wiki/Open_Content_Model_Example">Open Content Model Example</a> for a detailed example. 
		</p>
		<section>
			<h3>Summary:</h3>						
			<p>Requires the possibility to address a resource graph based on criteria unrelated to its rdf:type. This can be a general context, or a specific application function.</p>
			<p>Related to: <a>S4</a></p>
		</section>
		<section>
			<h3>Related Requirements:</h3>
			<p> ... </p>
		</section>
	</section>	

	<!-- User Story 25 -->
	<section>
		<h2><dfn>S25</dfn>: Primary Keys with URI Patterns</h2>
		<p>
			It is very common to have a single property that uniquely identifies instances of a given class. For example, when you import legacy data from a spreadsheet, it should be possible to automatically produce URIs based on a given primary key column. The proposed solution here is to define a standard vocabulary to represent the primary key and a suitable URI pattern. This information can then be used both for constraint checking of existing instances, and to construct new (valid) instances. One requirement here is advanced string processing, including the ability to turn a partial URI and a literal value into a new URI.
		</p>
		<p>
			Details: <a href="https://www.w3.org/2014/data-shapes/wiki/Primary_Keys_with_URI_Pattern">Primary Keys with URI Pattern</a>
		</p>
		<section>
			<h3>Summary:</h3>						
			<p>Requires The ability to create IRIs from non-IRI identifiers.</p>
		</section>
		<section>
			<h3>Related Requirements:</h3>
			<p> ... </p>
		</section>
	</section>	

	<!-- User Story 26 -->
	<section>
		<h2><dfn>S26</dfn>: rdf:Lists and ordered data</h2>
		<p>
			Can we express validating rdf:Lists a in our framework? This is more than just a stresstest but a variation of this can be used to check whether all members of a list have certain characteristics.
		</p>
		<p>
			Libraries have a number of resources that are issued in ordered series. Any library may own or have access to some parts of the series, either sequential or with broken sequences. The list may be very long, and it is often necessary to display the list of items in order. The order can be nicely numerical, or not. Another ordered list use case is that of authors on academic journal articles. For reasons of attribution (and promotion!), the order of authors in article publishing can be significant. This is not a computable order (e.g. alphabetical by name). There are probably other cases, but essentially there will definitely be a need to have ordered lists for some data. Validation could be: the list must have a beginning and end; there can be/cannot be gaps in the list.
		</p>
		<p>
			Details: <a href="https://www.w3.org/2014/data-shapes/wiki/Rdf:List_Stresstest">rdf:List Stresstest</a>
		</p>
		<section>
			<h3>Summary:</h3>						
			<p>Requires the possibility to define ordered and unordered lists of properties, including attributes like begin_element, end_element, etc.</p>
		</section>
		<section>
			<h3>Related Requirements:</h3>
			<p> ... </p>
		</section>
	</section>	

	<!-- User Story 27 -->
	<section>
		<h2><dfn>S27</dfn>: Relationships between values of multiple properties</h2>
		<p>
			Cultural heritage data is created in a distributed way, so when data is gathered together in a single aggregation, quite a bit of checking must be done. One of the key aspects of CH data is the identification of persons and subjects, in particular relating them to historical contexts. For persons, a key context is their own birth and death dates; for events, there is often a date range representing a beginning and end of the event. In addition, there are cultural heritage objects that exist over a span of time (serial publications, for example). In each of these cases, it is desirable to validate the relationship of the values of properties that have temporal or other ordered characteristics. 
		</p>
		<p>
			Details: <a href="https://www.w3.org/2014/data-shapes/wiki/Constraining_the_order_of_different_properties">Relationships between values of different properties</a>
		</p>
		<section>
			<h3>Summary:</h3>						
			<p>Requires ability to perform comparisons on the values in selected sets of properties. For example, to compare the value of properties representing birth date and death date to validate that birthdate precedes death date. Similar tests may be needed within workflows, for example to check that step one is completed before step two.</p>
			<p>Related to: Constraints over properties of the same node.</p>
		</section>
		<section>
			<h3>Related Requirements:</h3>
			<p> ... </p>
		</section>
	</section>	

	<!-- User Story 28 -->
	<section>
		<h2><dfn>S28</dfn>: Self-Describing Linked Data Resources</h2>
		<p>
			In Linked Data related information is accessed by URI dereferencing. The information that is accessible this way may represent facts about a particular resource, but also typing information for the resource. The types can themselves be used in a similar way to find the ontology describing the resource. It should be possible to use these same mechanisms to find constraints on the information provided about the resource.
		</p>
		<p>
			For example, the ontology could include constraints or could point to another document that includes constraints. Or the first document accessed might include constraints or point to another document that includes constraints. 
		</p>
		<p>
			DCMI story: For some properties there is a requirement that the value IRI resolve to a resource that is a skos:Concept. The resource value is not limited to a particular skos:Concept scheme. 
		</p>
		<section>
			<h3>Summary:</h3>						
			<p>1. The validation language must be able to make use of information obtained through dereferencing of the property IRI.*</p>
			<p>2. The validation language must be able to define validation for information received from a dereferencing of the value IRI, e.g. that the value is a member of a skos:ConceptScheme. </p>
			<p>* #1 strikes me as problematic since what is returned will be RDF/OWL with open world semantics.</p>
		</section>
		<section>
			<h3>Related Requirements:</h3>
			<p> ... </p>
		</section>
	</section>	

	<!-- User Story 29 -->
	<section>
		<h2><dfn>S29</dfn>: Describing interoperable, hypermedia-driven Web APIs (with Hydra)</h2>
		<p>
			<a href="http://www.hydra-cg.com/">Hydra</a> is a lightweight vocabulary to create hypermedia-driven Web APIs. By specifying a number of concepts commonly used in Web APIs it enables the creation of generic API clients. The Hydra core vocabulary can be used to define classes and "supported properties" which carry additional metadata such as whether the property is required and whether it is read-only.
		</p>
		<p>
			This feels very similar to the OSLC Resource Shapes story and uses similar constructs. It is also possible to express the supported properties as a SPIN constraint check, as implemented here: http://topbraid.org/spin/spinhydr
		</p>
		<section>
			<h3>Summary:</h3>						
			<p>Requires the possibility to define a set of routines or concepts that will fulfil commonly required validation tasks, with perhaps some selectable options. </p>
		</section>
		<section>
			<h3>Related Requirements:</h3>
			<p> ... </p>
		</section>
	</section>	

	<!-- User Story 30 -->
	<section>
		<h2><dfn>S30</dfn>: PROV Constraints</h2>
		<p>
			The <a href="http://www.w3.org/TR/prov-overview">PROV Family of Documents</a> defines a model, corresponding serializations and other supporting definitions to enable the inter-operable interchange of provenance information in heterogeneous environments such as the Web. One of these documents is a <a href="http://www.w3.org/TR/2013/REC-prov-constraints-20130430/">library of Constraints</a> which defines valid PROV instances. The actual validation process is quite complex and requires a normalization step that can be compared to rules. Various implementations of this validation process exist, including a set of SPARQL INSERT/SELECT queries sequenced by a <a href="https://github.com/pgroth/prov-check/blob/master/provcheck/provconstraints.py">Python script</a>, an <a href="https://provenance.ecs.soton.ac.uk/validator/view/validator.html">implementation in Java</a> and in <a href="https://github.com/jamescheney/prov-constraints">Prolog</a>. Stardog also defines an <a href="http://docs.stardog.com/admin/#sd-Archetypes">"archetype"</a> for PROV, which seems to be implemented in SPARQL using their ICV engine. 
		</p>
		<section>
			<h3>Summary:</h3>						
			<p>Requires support of PROV Constraints.</p>
			<p>Requires a mechanisms to define rules within shape definitions.</p>
		</section>
		<section>
			<h3>Related Requirements:</h3>
			<p> ... </p>
		</section>
	</section>	

	<!-- User Story 31 -->
	<section>
		<h2><dfn>S31</dfn>: LDP: POST content to Container of a certain shape</h2>
		<p>
			Some simple LDP server implementations may be based on lightweight app server technology and only deal with JSON(-LD) and Turtle representations for their LDP RDF Sources (LDP-RS) on top of an existing application, say Bugzilla. As a client implementer, I may have a simple JavaScript application that consumes and produces JSON-LD. I want to have a way to programmatically provide the end-user with a simple form to create new resources and also a way to potential auto-prefill this form based on data from current context.
		</p>
		<p>
			LDP defines some behavior when a POST fails to a ldp:Container, by outlining expected status codes and additional hints that could be found in either the response body of the HTTP POST request or a response header (such as: Link relation of "http://www.w3.org/ns/ldp#constrainedBy". A client can proactively request headers (instead of trying the POST and it fails) by performing an HTTP HEAD or OPTIONS request on the container URL and inspecting the link relation for "constrainedBy". Typical constraints are: a) not necessarily based on type b) sometimes limited to the action of creation and may not apply to other states of the resource.
		</p>
		<p>
			Current gap is whatever is at the end of the "constrainedBy" link, could be anything: HTML, OSLC Resource Shapes, SPIN. The LDP WG discussed a need to have something a bit more formalized and deferred making any recommendation looking to apply these requirements unto the Data Shapes work.Once it matures, and meets the requirements, LDP could provide a recommendation for it then. 
		</p>
		<section>
			<h3>Summary:</h3>						
			<p>covered in <a>S11</a>?</p>
		</section>
		<section>
			<h3>Related Requirements:</h3>
			<p> ... </p>
		</section>
	</section>	

	<!-- User Story 32 -->
	<section>
		<h2><dfn>S32</dfn>: Non-SPARQL based solution to express constraints between different properties</h2>
		<p>
			Consider the case of clients consuming RDF resources, interfacing with an LDP container, needs to work in a disconnected mode (the client being a Workers mobile device where the work zone has no connectivity). The client needs to allow workers to create entries locally in the device to mark completion of different stages of the work. These entries will get synched up with the LDP container at a later time, when the device gets connectivity back. Prior to that, when the client is in disconnected mode, the client software needs to perform a range of validations on the users entries to reduce the probabilty of an invalid entry.
		</p>
		<p>
			In addition to the basic data type/required/cardinality "stand alone" validations, the client needs to validate constraints between different properties:

			<ul>
				<li>start time less than end time</li>
				<li>if end time is not specified, the status of the "work" should be "In Progress"</li>
				<li>if status is "Complete" end time is required.</li>
			</ul>

			The client side does not have access to any triple store/LDP container. If these validations can be expressed in a higher level language which makes it simpler for clients to implement them constraint systems will be useful in more places. 
		</p>
		<section>
			<h3>Summary:</h3>						
			<p>Expresses the requirement to be able to define constraints over more than one property.
				E.g., value of property start time must be less than value of property end time.</p>
				<p>Those interdependencies between properties of the same RDF node should be expressible in a higher level language.</p>
				Related to: <a>S27</a>
			</section>
			<section>
				<h3>Related Requirements:</h3>
				<p> ... </p>
			</section>
		</section>	

		<!-- User Story 33 -->
		<section>
			<h2><dfn>S33</dfn>: Structural validation for queriability</h2>
			<p>
				Patient data (all data) is frequently full of structural errors. Statistical queries over malformed data leads to misinterpretation and inaccurate conclusions. Shapes can be used to sequester well-formed data for simpler analysis.
			</p>
			<p>
				Consider a schema where a medical procedure should have no more than one outcome. Accidental double entry occurs when e.g. a clinician and her assistant both enter outcomes into the database: 

				<pre id="example5" class="example highlight">
					_:Bob :hadIntervention [
					:performedProcedure [ a bridg:PerformedProcedure ;
					:definedBy [ :coding term:MarrowTransplant ; :location terms:Manubrium ] ];
					:assessmentTest     [ a bridg:PerformedObservation ;
					:definedBy [ :coding term:TumorMarkerTest ; :evaluator &lt;LabX&gt; ] ;
					:result    [ :coding term:ImprovedToNormal ; :assessedBy clinic:doctor7 ],
					[ :coding term:ImprovedToNormal ; :assessedBy clinic:doctor7 ]
					]
					] .
				</pre>
				The obvious SPARQL query on this will improperly weight this as two positive outcomes: 
				<pre id="example6" class="example highlight">
					SELECT ?location ?result (COUNT(*) AS ?count)
					WHERE {
					?who :hadIntervention [
					:performedProcedure [ :definedBy [ :coding term:MarrowTransplant ; :location ?location ] ];
					:assessmentTest     [ :definedBy [ :coding term:TumorMarkerTest ] ;
					:result    [ :coding ?result ] ]
					]
				} GROUP BY ?result ?location
			</pre>
			(This is a slight simplification for the sake of readability. In practice, an auxilliary hierarchy identifies multiple codes as positive outcomes, e.g. <strong>term:ImprovedToNormal</strong> and <strong>term2:ClinicalCure</strong>, but the effect is the same as described here.) 
		</p>
		<section>
			<h3>Summary:</h3>						
			<p>Requires the ability to perform structural validation over RDF data. Being able to select subsets of data related to 
				an RDF node and thus, define a well-formed/cleansed representation of that node (which is represented as shape), allows to 
				improve the quality of data as well as its queriability. </p>
			</section>
			<section>
				<h3>Related Requirements:</h3>
				<p> ... </p>
			</section>
		</section>	

		<!-- User Story 34 -->
		<section>
			<h2><dfn>S34</dfn>: Large-scale dataset validation</h2>
			<p>
				A publisher has a very large RDF Database (millions - billions triples) and wants to define multiple shapes for the data that will be checked at regular intervals. To make this process effective 1) validation must run within a reasonable time-span and 2) it must be possible to determine just what violations were found, i.e., just a TRUE/FALSE result is inadequate.
			</p>
			<section>
				<h3>Summary:</h3>					
				<p>Basically a repetition of <a>S3</a> with additional requirements regarding the validation performance. </p>
			</section>
			<section>
				<h3>Related Requirements:</h3>
				<p> ... </p>
			</section>
		</section>	

		<!-- User Story 35 -->
		<section>
			<h2><dfn>S35</dfn>: Describe disconnected graphs</h2>
			<p>
				In general, the RDF representation of an information resource may be a disconnected graph in the sense that the set of nodes in the graph may be partitioned into two disjoint subsets A and B such that there is no undirected path that starts in A and ends in B. The shape language must be able to describe such graphs. For example, consider the following JSON-LD representation of the Access Context List resource specified in <a href="http://open-services.net/wiki/core/IndexableLinkedDataProvider-2.0/">OSLC Indexable Linked Data Provider Specification V2.0: </a>

				<pre id="example7" class="example highlight">
					{
					"@context": {
					"acc": "http://open-services.net/ns/core/acc#",
					"id": "@id",
					"type": "@type",
					"title": "http://purl.org/dc/terms/title",
					"description": "http://purl.org/dc/terms/description"
				},
				"@graph": [{
				"id": "https://a.example.com/acclist",
				"type": "acc:AccessContextList"
			}, {
			"id": "https://a.example.com/acclist#alpha",
			"type": "acc:AccessContext",
			"title": "Alpha",
			"description": "Resources for Alpha project"
		}, {
		"id": "https://a.example.com/acclist#beta",
		"type": "acc:AccessContext",
		"title": "Beta",
		"description": "Resources for Beta project"
	}]
}
</pre>
There is no path from the acc:AccessContextList node to either of the acc:AccessContext nodes. There is an implicit containment relation of acc:AccessContext nodes in the acc:AccessContextList by virtue of these nodes being in the same information resource. However, the designers of this representation were attempting to eliminate clutter and appeal to Javascript developers, so they did not define explicit containment triples. 
</p>
<p>
	This user story is motivated by Linked Data and how information resources are created (e.g. via HTTP POST) or modified (e.g. via HTTP PUT). In these situations, the body of the HTTP request has an RDF content type (RDF/XML, Turtle, JSON-LD, etc.). The server typically needs to verify that the body of the request satisfies some application-specific constraints. If the request does not satisfy the constraints them it will fail the request and respond with 400 Bad Request or some similar response.
</p>
<p>
	This user story draws attention to the fact that RDF content is in general a graph. The concept of RDF graph is defined in <a href="http://www.w3.org/TR/rdf11-concepts/#section-rdf-graph">http://www.w3.org/TR/rdf11-concepts/#section-rdf-graph</a>. A general RDF graph may not be connected and in fact disconnected RDF graphs do appear in real-world Linked Data specifications. Therefore, the output of this workgroup must support the description of constraints on general RDF graphs, connected or not. 
</p>
<p>
	Some of the proposed solutions (Resource Shapes, ShEx, SPIN) appear to have an implicit assumption that the only RDF graphs of interest to this workgroup are like programming language data structures in the sense that there is a distinguished root node which is the subject of triples that define either literal properties or links to other subjects, which may in turn have literal properties or links to further subjects, or so forth. The implication is that all the nodes of interest are connected to the root node. Therefore, these proposals are incapable of describing disconnected graphs. The point of this user story is to provide evidence that disconnected graphs are of interest. It also attempts to make the point that the output of this workgroup should be applicable to general RDF graphs and not just some subset of graphs that follows some popular design pattern.
</p>
<p>
	The example is taken from a specification related to access control. A conformant access control service must host an access control list resource that supports HTTP GET requests. The response to an HTTP GET request have a response body whose content type is application/ld+json, i.e. JSON-LD. An example is given below. In this example, there is a distinguished root node, i.e. the node of type acc:AccessContextList, but it is not connected to the other nodes of interest, i.e. the nodes of type acc:AccessContext.
</p>
<p>
	An informal specification for valid RDF graphs is as follows: "Let X be the URI of an access control list information resource. Its RDF graph must must contain X as a resource node. X must have type acc:AccessContextList. X must have a string-valued dcterms:title property and a string-valued dcterms:description property. In addition, the graph may contain zero or more other resource nodes (URIs) of type acc:AccessContext. Each of these other nodes must have a string-valued dcterms:title property and a string-valued dcterms:description property. The graph may contain other triples."
</p>
<p>
	This user story does not propose that a shape language must be able to distinguish between connected and disconnected graphs. 
</p>
<section>
	<h3>Summary:</h3>		
	<p>States the requirement, that constraints over RDF graphs must be describable for both, disconnected and connected ones. </p>
</section>
<section>
	<h3>Related Requirements:</h3>
	<p> ... </p>
</section>
</section>	

<!-- User Story 36 -->
<section>
	<h2><dfn>S36</dfn>: Support use of inverse properties</h2>
	<p>
		In some cases the best RDF representation of a property-value pair may reuse a pre-existing property in which the described resource is the object and the property value is the subject. The reuse of properties is a best practice for enabling data interoperability. The fact that a pre-existing property might have the opposite direction should not be used as a justification for the creation of a new inverse property. In fact, the existence of both inverse and direct properties makes writing efficient queries more difficult since both the inverse and the direct property must be included in the query.
	</p>
	<p>
		For example, suppose we are describing test cases and want to express the relations between test cases and the requirements that they validate. Further suppose that there is a pre-existing vocabulary for requirements that defines the property ex:isValidatedBy which asserts that the subject is validated by the object. In this case there is no need to define the inverse property ex:validates. Instead the representation of test case resources should use ex:isValidatedBy with the test case as the object and the requirement as the subject.
	</p>
	<p>
		This situation cannot be described by the current OSLC Shapes specification because that specification has a directional bias. OSLC Shapes describe properties of a given subject node, so inverse properties cannot be used. The OSLC Shape submission proposes a possible solution. See <a href="http://www.w3.org/Submission/shapes/#inverse-properties">http://www.w3.org/Submission/shapes/#inverse-properties</a>.
	</p>
	<section>
		<h3>Summary:</h3>					
		<p>For sake of simplicity, a potential constraint language shall allow the usage of properties in their inverse direction if 
			applicable. I.e. allowing the reuse of already defined properties (in an inverse manner) in a shape, even if the node the 
			respective shape is describing only occurs in the object position.</p>
		</section>
		<section>
			<h3>Related Requirements:</h3>
			<p> ... </p>
		</section>
	</section>	

	<!-- User Story 37 -->
	<section>
		<h2><dfn>S37</dfn>: Defining allowed/required values</h2>
		<p>
			The cultural heritage community has a large number of lists that control values for particular properties. These are similar to the DCMItypes, but some are quite extensive (>200 types of roles for Agents in relation to resources). There is also a concept of "authorities" which control the identities of people, places, subjects, organizations and even resources themselves. Many of these lists are centralized in major agencies (Library of Congress, Getty Art &amp; Architecture Archive, National Library of Medicine, and national libraries throughout the world). Not all have been defined in RDF or RDF/SKOS, but those that have can be identified by their IRI domain name and pattern. Validation tools need to restrict or check usage according to the rules of the agency creating and sharing the data. Some patterns of needed validation are: 

			<ol>
				<li>must be an IRI (not a literal)</li>
				<li>must be an IRI matching this pattern (e.g. http://id.loc.gov/authorities/names/)</li>
				<li>must be an IRI matching one of >1 patterns</li>
				<li>must be a (any) literal</li>
				<li>must be one of these literals ("red" "blue" "green")</li>
				<li>must be a typed literal of this type (e.g. XML dataType)</li>
				<li>literal must have a language code</li> 
			</ol>
			Some of these are conditional: for resources of type:A, property:P has allowed values a,b,c,f. 
		</p>
		<section>
			<h3>Summary:</h3>			
			<p>Requires the ability to constrain the potential values of properties of a shape. </p>
			<p>Related to: <a>S2</a>,<a>S8</a>,<a>S11</a>, <a>S23</a></p>
		</section>
		<section>
			<h3>Related Requirements:</h3>
			<p> ... </p>
		</section>
	</section>	

		<!-- User Story 38 
		<section>
			<h2><dfn>S38</dfn>: Describing and Validating Linked Data portals</h2>
				<p>A small company is specialized in the development of linked data portals. The contents of those portals are usually from statistical data that comes from Excel sheets and can easily be mapped to RDF Data Cube observations.</p>
				<p>The company needs a way to describe the model of the RDF graphs that need to be generated from the Excel sheets which will also be published as an SPARQL endpoint. Notice that those linked data portals could contain observations which will usually be instances of qb:Observation but can contain different properties.</p>
				<p>Some constraints could be, for example, that any observation has only one floating point value, or that any observation refers to one geographical area, one year, one indicator and one dataset. That those datasets refer to organizations and those organizations have one rdfs:label property in English, another in French, and another in Spanish, etc. </p>
				<p>In this context, the company is looking for a solution that can be easily understood by the team of developers which are familiar work with OO programming languages, relational databases, XML technologies and some basic RDF knowledge, but they are not familiar with other semantic web technologies like SPARQL, OWL, etc.</p>
				<p>The company also wants some solution that can be published and understood by external semantic web developers so they can easily know how to query the SPARQL endpoint.</p>
				<p>There is also a need that the solution can be machine processable, so the contents of the linked data portal can automatically be validated.</p>
				<p>Finally, the company would like to compare the schemas employed by the different linked data portals so they can check which are the differences between the RDF nodes that appear in those portals and they can even create new applications on top of the data aggregated by those portals.</p>
				<p>The company would also like to promote third party companies to be able to reuse the data available in those data portals so there could be third-party applications on top of them which could, for example, visualize or compare the different observations, create faceted browsers, search engines, etc. To that end, those third party companies need some way to query the schemas available in those partals and build those applications from those schemas. </p>
				<section><h3>Summary:</h3>						<p>Requires the ability to constrain only specific parts of an RDF node. For example, stating that any observation has only one floating point value, regardless its other properties/values.  </p>
					</div>
				</section>	-->

		<!-- User Story xx 
		<section>
			<h2><dfn>Sxx</dfn>: </h2>
				<p></p>
				<div class="note">
					<p>Just an example of a note.. if required</p>
				</div>
			</section>	-->
		</section>
	<!--
	<section>
		<h1 id="usecases">Use Cases</h1>
		<p>The following use cases are each derived from one or more of the user stories above. These use cases are explored in detail through their description and their related user stories and requirements. The examples they contain are included purely for illustrative purposes, and should not be interpreted normatively.</p>

		<!-- Use Case 1 
		<section>
			<h2>UC1: Defining constraints of properties or classes of an RDF graph</h2>
			<p> 
				This use case addresses the ability to constrain certain properties or classes of an RDF graph (i.e. defining their shape). 
				For example, one could formalize the requirement that each property must have an associated domain or that each node of type X 
				has to have required properties Y and Z. Being able to define and constrain shapes of entities in an RDF graph, allows 
				to check whether that graph is valid wrt. to those defined constraints.
			</p>
			<p>
				<strong>Related User Stories:</strong>
				<a>S1</a>,
				<a>S2</a>,	
			</p>
			<p>
				<strong>Related Requirements:</strong>
			</p>
		</section>
		
		<!-- Use Case 2 
		<section>
			<h2>UC2: Associating more than one shape to the same node of an RDF graph</h2>
			<p>
				This use case addresses the possibility to define more than one set of constraints (i.e. shape) for a single node of an RDF graph.
				In scenarios where nodes of an RDF graph might serve more than one role, this feature might be required.
			</p>
			<p>
				<strong>Related User Stories:</strong>
				<a>S4</a>
			</p>
			<p>
				<strong>Related Requirements:</strong>
			</p>
		</section>
		
		<!-- Use Case 3 
		<section>
			<h2>UC3: Encapsulating complex constraint definitions into macros</h2>
			<p>
				This use case addresses the possibility to encapsulate complex constraint definitions into macros thus, allow their reuse in other shape definitions as 
				well as avoid verbose constraint declarations.
			</p>
			<p>
				<strong>Related User Stories:</strong>
				<a>S16</a>
			</p>
			<p>
				<strong>Related Requirements:</strong>
			</p>
		</section>
		
		<!-- Use Case 3 
		<section>
			<h2>UC3: Associating more than one shape to the same node of an RDF graph</h2>
			<p>
				In scenarios where nodes of an RDF graph might serve more than one role, it  
			</p>
			<p>
				<strong>Related User Stories:</strong>
				<a>S4</a>
			</p>
			<p>
				<strong>Related Requirements:</strong>
			</p>
		</section
	</section>-->
	<section>
		<h1 id="requirements">Requirements</h1>
		<p>This section lists the requirements arising from the use-cases catalogued in this document. Specific requirements that have been de-prioritized or rejected have been left in the document for completeness, but are shown as struck out.</p>
		<dl>
			<!-- Requirement R1 -->
			<dt>
				<dfn>R2.1</dfn>: Higher-Level Language
			</dt>
			<dd>
				<p>
					<em>Constraints/shapes shall be specifiable in a higher-level language with 1. definitional capabilities, such as macro rolling up and naming, and 2. control infrastructure for, e.g., recursion.</em> 
				</p>
				<p>
					<strong>Motivation:</strong> 
					<a href="http://lelystad.informatik.uni-mannheim.de/rdf-validation/?q=node/127">Dublin Core Requirement 103</a>
				</p>
			</dd>

			<!-- Requirement R2 -->
			<dt>
				<dfn>R2.2</dfn>: Concise Language
			</dt>
			<dd>
				<p>
					<em>Constraints/shapes shall be specifiable in a concise language.</em> 
				</p>
				<p>
					<strong>Motivation:</strong> 
					<a href="http://lelystad.informatik.uni-mannheim.de/rdf-validation/?q=node/327">Dublin Core Requirement 184</a>
				</p>
			</dd>

			<!-- Requirement R3 -->
			<dt>
				<dfn>R2.3</dfn>: Addressability
			</dt>
			<dd>
				<p>
					<em>Collections of constraints/shapes may be addressable and discoverable. Individual constraints/shapes may be addressable and discoverable. </em> 
				</p>
				<p>
					<strong>Motivation:</strong> 
					<a href="http://lelystad.informatik.uni-mannheim.de/rdf-validation/?q=node/203">Dublin Core Requirement 147</a> and
					<a href="http://lelystad.informatik.uni-mannheim.de/rdf-validation/?q=node/204">Dublin Core Requirement 148</a>
				</p>
			</dd>

			<!-- Requirement R4 -->
			<dt>
				<dfn>R2.4</dfn>: Annotations
			</dt>
			<dd>
				<p>
					<em>Constraints/shapes may incorporate extra information that does not affect validation. It shall be possible to search for constraints/shapes with particular extra information. </em> 
				</p>
				<p>
					<strong>Motivation:</strong> 
					<a href="http://lelystad.informatik.uni-mannheim.de/rdf-validation/?q=node/415">Dublin Core Requirement 208</a>
				</p>
			</dd>


                <!-- Requirement R5 
                <dt>
					<dfn>R5</dfn>: Declarations of Member Properties at Classes
				</dt>
				<dd>
					<p>
						<em>The language needs to include an easy-to-use vocabulary to declare that a given property is associated with a class, e.g. to populate input forms with appropriate widgets but also constraint checking. In OO terms this is the declaration of a member, field, attribute or association.  </em> 
					</p>
                	<p>
                  		<strong>Motivation:</strong> 
                  		<a>S3</a>,
                  		<a>S10</a>,
                  		<a>S11</a>,
                  		<a>S12</a>,
                  		<a>S13</a>,
                  		<a>S15</a>,
                  		<a>S19</a>,
                  		<a>S20</a>,
                  		<a>S29</a>, and
                  		<a>S36</a>
                  	</p>
					<dl>
		                <!-- Requirement R5.1 
		                <dt>
							<dfn>R5.1</dfn>: Declarations of Property Min/Max Cardinality
						</dt>
						<dd>
							<p>
								<em>The stated values for a property may be limited by minimum/maximum cardinality, with typical patterns being [0..1], [1..1], [0..*] and [1..*]. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S10</a>,
		                  		<a>S11</a>,
		                  		<a>S13</a>,
		                  		<a>S19</a>, and
		                  		<a>S20</a>
		                  	</p>
		                </dd>

		                <!-- Requirement R5.2 
		                <dt>
							<dfn>R5.2</dfn>: Declarations of Property Value Type (Range)
						</dt>
						<dd>
							<p>
								<em>The values of a property on instances of a class may be limited by their value type, such as xsd:string or ex:Person. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S10</a>,
		                  		<a>S11</a>,
		                  		<a>S13</a>,
		                  		<a>S19</a>, and
		                  		<a>S20</a>
		                  	</p>
		                </dd>

		                <!-- Requirement R5.3 
		                <dt>
							<dfn>R5.3</dfn>: Declarations of Property's RDF Node Type (e.g. only IRIs are allowed)
						</dt>
						<dd>
							<p>
								<em>The values of a property on instances of a class may be limited by their RDF node type, e.g. IRI, BlankNode, Literal, or BlankNodeOrIRI (for completeness we may want to support all 7 combinations including Node as parent). </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S8</a>
		                  	</p>
		                </dd>

		                <!-- Requirement R5.4 
		                <dt>
							<dfn>R5.4</dfn>: Declarations of a Property Default Value
						</dt>
						<dd>
							<p>
								<em>It should be possible to declare the default value for a given property, e.g. so that input forms can be pre-populated and to insert a required property that is missing in a web service call. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S11</a>,
		                  		<a>S19</a>, and
		                  		<a>S20</a>
		                  	</p>
		                </dd>

		                <!-- Requirement R5.5 
		                <dt>
							<dfn>R5.5</dfn>: Declarations of Property Comment at Class
						</dt>
						<dd>
							<p>
								<em>It should be possible to provide human-readable descriptions of the role of a property in the context of a class, not just globally using triples that have the rdf:Property as subject. Multiple languages should be supported.</em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S11</a> and
		                  		<a>S19</a>
		                  	</p>
		                </dd>

		                <!-- Requirement R5.6 
		                <dt>
							<dfn>R5.6</dfn>: Declarations of Datatype Property Facets
						</dt>
						<dd>
							<p>
								<em>For datatype properties it should be possible to declare frequently needed "facets" to drive user interfaces and validate input against simple conditions, including min/max value, regular expressions, string length etc. similar to XSD datatypes.</em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S3</a>,
		                  		<a>S11</a>,
		                  		<a>S12</a>,
		                  		<a>S13</a>,,
		                  		<a>S19</a>,
		                  		<a>S20</a>, and
		                  		<a>S29</a>
		                  	</p>
		                </dd>

		                <!-- Requirement R5.7 
		                <dt>
							<dfn>R5.7</dfn>: Declarations of Property Value Enumerations
						</dt>
						<dd>
							<p>
								<em>It is a common requirement to narrow down the value space of a property by an exhaustive enumeration of the valid values (both literals or resource).  </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S3</a>,
		                  		<a>S11</a>, and
		                  		<a>S37</a>
		                  	</p>
		                </dd>

		                <!-- Requirement R5.8 
		                <dt>
							<dfn>R5.8</dfn>: Declarations of Properties Used in Inverse Direction
						</dt>
						<dd>
							<p>
								<em>In many cases properties are used bi-directionally and then accessed in the inverse direction, e.g. parent = ^child. There should be a way to declare value type, cardinality etc of those inverse relations without having to declare a new property URI. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S36</a> 
		                  	</p>
		                </dd>

		                <!-- Requirement R5.9 
		                <dt>
							<dfn>R5.9</dfn>: Declarations of Primary Key Properties
						</dt>
						<dd>
							<p>
								<em>It is often useful to declare a given (datatype) property as the "primary key" of a class, so that the system can enforce uniqueness and also automatically build URIs from user input and data imported from relational databases or spreadsheets. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S5</a> and
		                  		<a>S25</a>
		                  	</p>
		                </dd>
		            </dl>
		        </dd>
				
				<!-- Requirement R6 
                <dt>
					<dfn>R6</dfn>: Complex Constraints
				</dt>
				<dd>
					<p>
						<em>The language should allow users to implement constraints that check complex conditions, with an expressivity as covered by the following sub-requirements (e.g. basic graph patterns, string and mathematical operations and comparison of multiple values). </em> 
					</p>
                	<p>
                  		<strong>Motivation:</strong> 
                  		<a>S5</a>,
                  		<a>S21</a>,
                  		<a>S22</a>,
                  		<a>S23</a>,
                  		<a>S26</a>,
                  		<a>S27</a>, and
                  		<a>S30</a>
                  	</p>
                
					<dl>
		                <!-- Requirement R2.6.1 
		                <dt>
							<dfn>R2.6.1</dfn>: Expressivity - Basic Graph Patterns
						</dt>
						<dd>
							<p>
								<em>Many constraints require matching patterns within the graph, often represented via linked triple patterns (SPO) and property paths. Requires variable bindings for matching, so that multiple values can be compared with each other. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S1</a>,
		                  		<a>S5</a>,
		                  		<a>S17</a>,
		                  		<a>S21</a>, 
		                  		<a>S22</a>,
		                  		<a>S23</a>,
		                  		<a>S26</a>,
		                  		<a>S27</a>, and
		                  		<a>S30</a>
		                  	</p>
		                </dd>

		                <!-- Requirement R2.6.2 -->
		                <dt>
		                	<dfn>R2.6.2</dfn>: Expressivity - Non-Existance of Patterns
		                </dt>
		                <dd>
		                	<p>
		                		<em>Many constraints require that a certain pattern does not exist in the graph. </em> 
		                	</p>
		                	<p>
		                		<strong>Motivation:</strong> 
		                		<a>S1</a>,
		                		<a>S2</a>,
		                		<a>S22</a>, and
		                		<a>S23</a>
		                	</p>
		                </dd>

		                <!-- Requirement R2.6.3 -->
		                <dt>
		                	<dfn>R2.6.3</dfn>: Expressivity - String Operations
		                </dt>
		                <dd>
		                	<p>
		                		<em>Some constraints require building new strings out of other strings, and building new URIs out of other values. </em> 
		                	</p>
		                	<p>
		                		<strong>Motivation:</strong> 
		                		<a>S5</a> and
		                		<a>S23</a>
		                	</p>
		                </dd>

		                <!-- Requirement R2.6.4 -->
		                <dt>
		                	<dfn>R2.6.4</dfn>: Expressivity - Language Tags
		                </dt>
		                <dd>
		                	<p>
		                		<em>Some constraints require comparing language tags of RDF literals, e.g. to check that no language is used more than once per property. Also to produce multi-lingual error messages. </em> 
		                	</p>
		                	<p>
		                		<strong>Motivation:</strong> 
		                		<a>S21</a>
		                	</p>
		                </dd>

		                <!-- Requirement R2.6.5 -->
		                <dt>
		                	<dfn>R2.6.5</dfn>: Expressivity - Mathematical Operations
		                </dt>
		                <dd>
		                	<p>
		                		<em>Some constraints require mathematical calculations and comparisons, e.g. area = width * height. </em> 
		                	</p>
		                	<p>
		                		<strong>Motivation:</strong> 
		                		<a>S5</a>
		                	</p>
		                </dd>

		                <!-- Requirement R2.6.6 -->
		                <dt>
		                	<dfn>R2.6.6</dfn>: Expressivity - Literal Value Comparison
		                </dt>
		                <dd>
		                	<p>
		                		<em>Some constraints require operators such as <, <=, != etc, either against constants or other values that are dynamically retrieved at query time. Includes date/time comparison and functions such as NOW(). </em> 
		                	</p>
		                	<p>
		                		<strong>Motivation:</strong> 
		                		<a>S5</a>,
		                		<a>S21</a>, 
		                		<a>S22</a>,
		                		<a>S23</a>, and
		                		<a>S27</a>
		                	</p>
		                </dd>

		                <!-- Requirement R2.6.7 -->
		                <dt>
		                	<dfn>R2.6.7</dfn>: Expressivity - Logical Operators
		                </dt>
		                <dd>
		                	<p>
		                		<em>The language should make it possible to express the basic logical operators intersection, union and negation of conditions. </em> 
		                	</p>
		                	<p>
		                		<strong>Motivation:</strong> 
		                		<a>S5</a>,
		                		<a>S26</a>, and
		                		<a>S35</a>
		                	</p>
		                </dd>

		                <!-- Requirement R2.6.8 
		                <dt>
							<dfn>R2.6.8</dfn>: Expressivity - Transitive Property Traversal
						</dt>
						<dd>
							<p>
								<em>Some constraints need to be able to traverse a property transitively, especially rdfs:subClassOf but also other parent-child relationships</em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S16</a>,
		                  		<a>S23</a>, and
		                  		<a>S26</a>
		                  	</p>
		                </dd>

		                <!-- Requirement R2.6.9 
		                <dt>
							<dfn>R2.6.9</dfn>: Expressivity - Aggregations
						</dt>
						<dd>
							<p>
								<em>Some constraints require aggregating multiple values, especially via COUNT, MIN and MAX. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S22</a>
		                  	</p>
		                </dd>

		                <!-- Requirement R2.6.10 
		                <dt>
							<dfn>R2.6.10</dfn>: Expressivity - Named Graphs
						</dt>
						<dd>
							<p>
								<em>Some constraints require looking up information from other named graphs, for example to verify that certain values exist in a controlled vocabulary or background knowledge. This information is usually not explicitly imported into the query graph, and having all sub-graphs in the default query graph would be too inefficient.</em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S5</a>
		                  	</p>
		                </dd>

		                <!-- Requirement R2.6.11 
		                <dt>
							<dfn>R2.6.11</dfn>: Expressivity - Closed Shapes
						</dt>
						<dd>
							<p>
								<em>Some data recipients will not act as generic triple stores. "Closed shapes" reject any graph that has triples that do not match something in the shapes. (The control can probably be applied to the whole schema rather than individual shapes. At least, there's no use case or implementation experience to the contrary.) </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S4</a>
		                  	</p>
		                </dd>
		            </dl>
				</dd>
				
				<!-- Requirement R7 
                <dt>
					<dfn>R7</dfn>: Macro-Language Features
				</dt>
				<dd>
					<p>
						<em>The language should enable the definition of macros as short cuts to recurring patterns, and to enable inexperienced users define rich constraints. Macros should be high-level terms that improve overall readability, separation of concerns and maintainability. This overlaps with the already approved "Higher-Level Language". </em> 
					</p>
                	<p>
                  		<strong>Motivation:</strong> 
                  		<a>S5</a>,
                  		<a>S7</a>,
                  		<a>S16</a>,
                  		<a>S21</a>,
                  		<a>S27</a>,
                  		<a>S28</a>, and
                  		<a>S32</a>
                  	</p>
                
					<dl>
		                <!-- Requirement R2.7.1 
		                <dt>
							<dfn>R2.7.1</dfn>: Named Shapes
						</dt>
						<dd>
							<p>
								<em>It should be possible to encapsulate a group of constraints (a Shape) into a named entity, so that the Shape can be reused in multiple places, also across the Web</em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S7</a>,
		                  		<a>S16</a>, and
		                  		<a>S28</a>
		                  	</p>
		                </dd>
		                <!-- Requirement R2.7.2 
		                <dt>
							<dfn>R2.7.2</dfn>: Function and Property Macros
						</dt>
						<dd>
							<p>
								<em>In order to support maintainable and readable constraint definitions, it should be possible to encapsulate recurring patterns into named entities such as functions and dynamically computed properties. This requirement is orthogonal to almost every user story. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S5</a>,
		                  		<a>S16</a>, and
		                  		<a>S28</a>
		                  	</p>
		                </dd>
		                <!-- Requirement R2.7.3 -->
		                <dt>
		                	<dfn>R2.7.3</dfn>: Constraint Macros
		                </dt>
		                <dd>
		                	<p>
		                		<em>Some constraint patterns are recurring with only slight modifications. Example: SKOS constraints that multiple properties must be pairwise disjoint. The language should make it possible to encapsulate such recurring patterns in a parameterizable form.</em> 
		                	</p>
		                	<p>
		                		<strong>Motivation:</strong> 
		                		<a>S21</a>,
		                		<a>S27</a>, and
		                		<a>S28</a>
		                	</p>
		                </dd>
		                <!-- Requirement R2.7.4 
		                <dt>
							<dfn>R2.7.4</dfn>: Nested Constraint Macros
						</dt>
						<dd>
							<p>
								<em>It should be possible to combine the high-level terms of the constraint language into larger expressions using nested constraints. Examples of this include ShEx, Resource Shapes' oslc:valueShape and owl:allValuesFrom. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S32</a> and
		                  		<a>S33</a>
		                  	</p>
		                </dd>
					</dl>
				</dd>
                <!-- Requirement R2.8 
                <dt>
					<dfn>R2.8</dfn>: Inheritance of Constraints
				</dt>
				<dd>
					<p>
						<em>According to the general semantics of RDF Schema (and the intuition of most users), any instance of a subclass is also an instance of a superclass. Therefore, any constraints defined for the superclass also need to apply to the subclasses. Subclasses can only further constrain, i.e. narrow down, inherited constraints. </em> 
					</p>
                	<p>
                  		<strong>Motivation:</strong> 
                  		<a>S2</a>,
                  		<a>S5</a>,
                  		<a>S10</a>,
                  		<a>S11</a>,
                  		<a>S19</a>,
                  		<a>S20</a>,
                  		<a>S24</a>,
                  		<a>S25</a>,
                  		<a>S27</a>,
                  		<a>S28</a>, and
                  		<a>S29</a>
                  	</p>
                </dd>
                <!-- Requirement R2.9 
                <dt>
					<dfn>R2.9</dfn>: Static Constraints
				</dt>
				<dd>
					<p>
						<em>It should be possible to specify constraint conditions that need to be checked "globally" for a whole graph, without referring to a specific set of resources or class. In programming languages such global entities are often called "static", thus the name. </em> 
					</p>
                	<p>
                  		<strong>Motivation:</strong> 
                  		<a>S35</a>
                  	</p>
                </dd>
                <!-- Requirement R2.10 -->
                <dt>
                	<dfn>R2.10</dfn>: Vocabulary for Constraint Violations
                </dt>
                <dd>
                	<p>
                		<em>Instead of just reporting yes/no, the language needs to be able to return more meaningful messages including severity levels, human-readable error descriptions and pointers at specific patterns in the graph. </em> 
                	</p>
                	<p>
                		<strong>Motivation:</strong> 
                		<a>S3</a>,
                		<a>S34</a>, and
                		(almost every other User Story)
                	</p>

                </dd>
                  		<!-- Requirement R2.10.1 
                		<dt>
							<dfn>R2.10.1</dfn>: Severity Levels
						</dt>
						<dd>
							<p>
								<em>The language should include at least the following severity levels: Warning, Error, Fatal Error (Fatal means evaluation can stop). Maybe we also need Info/Debug? </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S3</a>
		                  	</p>
		                </dd>
		                <!-- Requirement R2.10.2 
		                <dt>
							<dfn>R2.10.2</dfn>: Human-readable Violation Messages
						</dt>
						<dd>
							<p>
								<em>The language should make it possible for constraint checks to create human-readable violation messages, and ideally those should be generated dynamically from with the current context. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S3</a>
		                  	</p>
		                </dd>
		                <!-- Requirement R2.10.3 -->
		                <dt>
		                	<dfn>R2.10.3</dfn>: Constraint Violations should point at Specific Nodes
		                </dt>
		                <dd>
		                	<p>
		                		<em>The language should make it possible for authors of constraint checks to produce pointers at specific nodes and graph fragments that caused the violation. Typical examples of such information includes the starting point (root node), a path from the root, and specific values that caused the problem. </em> 
		                	</p>
		                	<p>
		                		<strong>Motivation:</strong> 
		                		<a>S3</a>
		                	</p>
		                </dd>


                <!-- Requirement R2.11 
                <dt>
					<dfn>R2.11</dfn>: Modularization
				</dt>
				<dd>
					<p>
						<em>The language should support organizing constraint declarations in different groups, modules or graphs, and provide mechanisms to allow modules to point to each other. </em> 
					</p>
                	<p>
                  		<strong>Motivation:</strong> 
                  		<a>S1</a>,
                  		<a>S36</a>, and
                  		<a>S37</a>
                  	</p>
                  	<dl>
                  		<!-- Requirement R2.11.1 
                		<dt>
							<dfn>R2.11.1</dfn>: Organizing Constraints in Named Graphs
						</dt>
						<dd>
							<p>
								<em>The language should support using the standard linked data concept of named graphs (datasets) to organize constraint declarations. Such named graphs have a URI that is resolvable in the context of the application (e.g. on the public web via HTTP). Applications may define their own look-up mechanism to resolve such named graphs (e.g. to local database graphs or files). This includes the ability to separate a domain model from constraints. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S5</a>,
		                  		<a>S6</a>,
		                  		<a>S7</a>,
		                  		<a>S13</a>,
		                  		<a>S15</a>,
		                  		<a>S20</a>,
		                  		<a>S24</a>, and
		                  		<a>S28</a>
		                  	</p>
		                </dd>
		                <!-- Requirement R2.11.2 
		                <dt>
							<dfn>R2.11.2</dfn>: Including Named Graphs for Query Evaluation
						</dt>
						<dd>
							<p>
								<em>The language should support including named graphs (similar to owl:imports) so that all constraints from the (transitively) included graphs are also applied for evaluation. Conceptually, all included graphs are a union graph that becomes the default query graph of the constraint evaluation. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S5</a>,
		                  		<a>S6</a>,
		                  		<a>S13</a>,
		                  		<a>S20</a>, and
		                  		<a>S24</a>
		                  	</p>
		                </dd>
		                <!-- Requirement R2.11.3 
		                <dt>
							<dfn>R2.11.3</dfn>: Possible Separation of Query and Library Graphs
						</dt>
						<dd>
							<p>
								<em>The language should be efficient to execute on databases, so that the execution engine can exploit native optimizations from the database. Some data that is needed for execution (such as the constraint definitions themselves, macros and functions) may not be present on each graph on the database. Therefore, it should be possible to separate the graphs needed at constraint evaluation time from those graphs that hold the complete definition of the constraint checking context. A possible solution would be to have another kind of include mechanism that links a data graph with (macro) libraries. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S34</a>
		                  	</p>
		                </dd>
		                <!-- Requirement R2.11.4 
		                <dt>
							<dfn>R2.11.4</dfn>: Profiles
						</dt>
						<dd>
							<p>
								<em>The language should include a notion of profiles, so that certain applications with limited features can only use certain elements of the overall language. In particular, it should be easy to separate structural constraints (property declarations) from complex constraints (arbitrary SPARQL or nested constraint expressions) so that certain light-weight applications can make sense of a model without a full SPARQL processor. </em> 
							</p>
		                	<p>
		                  		<strong>Motivation:</strong> 
		                  		<a>S11</a>,
		                  		<a>S19</a> and
		                  		<a>S32</a>
		                  	</p>
		                </dd>
		                <!-- Requirement R2.11.5 -->
		                <dt>
		                	<dfn>R2.11.5</dfn>: Evaluating Constraints for a Single Node Only
		                </dt>
		                <dd>
		                	<p>
		                		<em>It should be possible to validate constraints on a single node in a graph. This may be impossible to implement 100% correctly, because sometimes a change to a resource invalidates conditions in a very different place in the graph. However, the language could propose a framework that identifies those constraints that SHOULD be checked when a given node is evaluated, e.g. by following its rdf:type and the superclasses of that. </em> 
		                	</p>
		                	<p>
		                		<strong>Motivation:</strong> 
		                		(Orthogonal to basically all stories) 
		                	</p>
		                </dd>
		            </dl>
		        </dd>
		    </dl>

		</section>
	<!--
	<pre id="example1" class="example highlight">
					@prefix ro:  &lt;http://purl.org/wf4ever/ro#&gt; .
					@prefix dct: &lt;http://purl.org/dc/terms/&gt; .
					@prefix ore: &lt;http://www.openarchives.org/ore/&gt; .
					@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .

					&lt;&gt; a ro:ResearchObject, ore:Aggregation ;
						dct:created "2012-12-01"^^xsd:dateTime .
				</pre>
				<div class="note">
						<p>
							The basic facilities provided by <a href="#ch_domain"><code>rdfs:domain</code></a>
							and <a href="#ch_range"><code>rdfs:range</code></a> do not provide any
							direct way to indicate property restrictions that are local to a class.
							Although it is possible to combine use <a href="#ch_domain"><code>rdfs:domain</code></a>
							and <a href="#ch_range"><code>rdfs:range</code></a> with sub-property
							hierarchies, direct support for such declarations are provided by richer
							Web Ontology languages such as OWL.
					</p>
				</div>-->
				<section class='appendix'>
					<h2>Acknowledgements</h2>
					<p>
						We would like to acknowledge the contributions of user story authors: Dean Allemang, Anamitra Bhattacharyya, Karen Coyle, Nick Crossley, Michel Dumontier, Jose Emilio Labra Gayo, Sandro Hawke, Dimitris Kontokostas, Holger Knublauch, David Martin, Dave McComb, Peter F. Patel-Schneider, Axel Polleres, Eric Prud'hommeaux, Arthur Ryman, Steve Speicher, and Simon Steyskal.
					</p>
				</section>

				<section id='tof'></section>
			</body>
			</html>
